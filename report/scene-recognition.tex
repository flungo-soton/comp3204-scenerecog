%%% Preamble
\documentclass[paper=a4]{article}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\begin{document}
\title{Scene Recognition}
\author{Jianzu Guan (jg17g13) and Fabrizio Lungo (fl4g12)}
\maketitle

\section{Overview}

In this project we will be implementing algorithms in order to classify images based on the scene that they show. We have bee provided with 1500 labelled training images from 15 classes (100 images per class). If we were to randomly guess and assign classes to the images, we would expect to have an accuracy of approximately $6.67\%$.

The implementations will be implemented in Java and utilising the OpenIMAJ library as best as possible.

\section{Framework}

In order to assist with the running and evaluation of the classification implementations and reduce code duplication a framework has been implemented.

To get the data (either from the remote URL or locally, if available) a \texttt{DatasetUtil} is provided which is able to give the labelled training set and unlabelled test sets.

% TODO: Actually produce classifier, need to update code and class names to reflect this.
All of the classifiers eventually produce an \texttt{Annotator} which assigns a class to each image. This can be used to abstract the evaluation and usage of the implementation. Since different assigners have different methods to be trained, this is abstracted through an \texttt{AnnotatorWrapper} which provides a train method taking the unlabelled dataset. The wrapper can handle any implementation specific features and provide an interface which is generic for all tests.

In order to write the outputs of classifications to the respective output file in the format required, the \texttt{GroupWriter} class takes a \texttt{GroupedDataset} and writes the classifications in the required format.

An abstracted base class \texttt{Classification} has been created which is able to train, test and evaluate an \texttt{Annotator}. An abstract \texttt{getAnnotatorWrapper} method is declared which should be fulfilled by the specific classifications. It uses the \texttt{AnnotatorWrapper} returned to interface with the specific annotator to evaluate, train and test.

Evaluation uses the \texttt{ClassificationEvaluator} provided by OpenIMAJ in order to validate the training data. The data is split using a \texttt{StratifiedGroupedRandomisedPercentageHoldOut} which splits the data ensuring that the training and validation ratio is consistent for each class. We have chosen to split the data with $85\%$ for training and $15\%$ for validation.

The train and test will use all of the training data and then test the unlabelled test data using a \texttt{DatasetClassifier} to classify the data into a \texttt{GroupedDataset} and the appropriate \texttt{GroupWriter} to write the output the results to file. In order to maintain file names, the images from the testing data are wrapped with \texttt{IdentifyableObject}.

All classes have been implemented using Java Generics where appropriate in order to make them as reusable and robust as possible.

\section{Implementations}

\subsection{Tiny Images}

The tiny images classifier reduces the size of the images to a $16 \times 16$ image by cropping the center and resizing. These tiny images can then be used a feature vectors of $256$ values. By reducing the images, it results in the averaging of values in regions in the image and using K-nearest-neighbour, these images can be classified to the class of the images from the training set which they most closely resemble.

With this implementation, we were able to achieve an accuracy of $28.0\%$ (using $k=1$) which is better than random guessing but is still not accurate enough for use in any meaningful applications.

This classifier performs poorly as it relies on similar scenes have similar pixel values in the same pixel location. Where this may be the case for certain scenes, such as streets  which have dark tarmac in the bottom middle with light buildings on either side and highways with dark tarmac containing white patches and light sky (both of which achieved 66.7\% accuracy within their class).

Various $k$ values were tested and in general lower values of $k$ seemed to perform better and hence why $k=1$ was chosen. Using tiny images does not cluster well and so the neighbourhood of the vector seemed to have little significance. Using $k=1$ meant that if it was similar to one of the training images it would be classified the same. This may suggest that with more unseen data, $k=1$ may perform worse than a greater value.

\subsection{Bag of Visual Words}

The Bag of Visual Word classifier extracts features from the images and utilises the \texttt{LiblinearAnnotator} to build a linear classifier that uses these features to classify.

This classifier has 2 stages as part of its training. First it trains a \texttt{HardAssigner} to be able to assign patches to a closest match in a codebook. The codebook is trained by splitting images into patches using a specified  \texttt{patchSize} and \texttt{patchStep}. For a single image this can create hundreds of patches which would make reducing to a set of core features much slower and possibly require a higher $k$ value for the clustering so a fixed number of \texttt{patchesPerImage} are kept from each image. 

Once patches have been collected from the images, $k$-means clustering is performed to reduce to a specified number of centroids which provides the \texttt{HardAssigner} that will be used to assign patches to their closest centroid.

This \texttt{HardAssigner} used as part of a feature extractor which extracts a feature representing the number of occurrences of the visual words. The \texttt{BoVWExtractor} uses the trained \texttt{HardAssigner} to extract features by iterating through all patches and assigning to one of the centroids in the \texttt{HardAssigner}'s code book. The resulting feature vector is a sparse vector representing the number of times that a feature form the codebook was found in patches from the image.

The \texttt{LiblinearAnnotator} then uses the \texttt{BoVWExtractor} to extract the features form each image. During the training phase, it creates a L2-regularized L2-loss support vector classification between these features and the class of the images that contain the respective feature so that it is able to identify images.

The BoVW classifier uses 4 parameters to configure its running, \texttt{patchSize}, \texttt{patchStep}, \texttt{patchesPerImage} and \texttt{kMeans}. All of these parameters are used in the construction of the codebook and can be tweaked to optimise for speed or performance. For our final run, we decided upon using \texttt{patchSize = 8}, \texttt{patchStep = 4}, \texttt{patchesPerImage = 10} and \texttt{kMeans = 500}. Using these values provides a reasonable patch size with overlap for between the patches that are extracted and should make finding relevant matching patches effective. With these parameters, we were able to achieve an accuracy of $53.8\%$. %TODO Is this correct?

\subsection{Pyramid Histogram of Words}

The Pyramid Histogram of Words classifier extracts dense SIFT features from the images and utilises the \texttt{LiblinearAnnotator} to build a linear classifier that uses these features to classify.

The classifier utilises a \texttt{PyramidDenseSIFT} which uses bins of a specified size and step, similar to the patches used in the Bag of Visual Word classifier. The pyramid aspect runs the algorithm at various scales of the image with Gaussian smoothing to provide scale invariance of the features. This is then used to extract a list of local SIFT key points that have an energy above a certain threshold.

A \texttt{HardAssigner} is created to assign features their closest match in the list of features that were evaluated during training. To perform this assignment an approximate \texttt{ByteKMeans} classification is performed using an ensemble of KD-Trees to perform nearest-neighbour lookup which is trained with the features to find a specified number of visual words (centroids in the K-means classification).

To extract features from images, the \texttt{PHoWExtractor} analyses the image to find its SIFT features using the same \texttt{PyramidDenseSIFT} that was used to learn the visual words. These features are then assigned to their closest match using the \texttt{HardAssigner} as part of a \texttt{BagOfVisualWords}. To improve the accuracy, we wrap the \texttt{BagOfVisualWords} with a \texttt{PyramidSpatialAggregator} that split the image into a specified number of sections for each level of its pyramid. Features are spatially aggregated into histograms by the \texttt{BagOfVisualWords}.

% Homogeneous

As with the Bag of Visual Word classifier, a \texttt{LiblinearAnnotator} is used to construct the L2-regularized L2-loss support vector classifier for classifying images, but using the \texttt{PHoWExtractor} instead of the \texttt{BoVWExtractor} which is wrapped with a \texttt{HomogeneousKernelMap} to improve accuracy.

The implementation we ran for classification is configured with the following:
\begin{itemize}
\item \texttt{HardAssigner} constructed with a maximum of $10000$ features which are grouped into $600$ $k$-mean centroids.
\item \texttt{PyramidDenseSIFT} configured with bin size of $7$ and step $3$ using a pyramid with magnification factor of $6$ and scales of $4$, $6$, $8$ and $10$.
\item \texttt{HomogeneousKernelMap} configured to use the \texttt{Chi2} kernel type and a rectangular window type.
\end{itemize}

With this classifier, we were able to achieve an accuracy of $77.3\%$ which is much higher than that gained from the Tiny Images and BoVW classifiers. For certain classes such as forest, tall building, suburb and coast, this classifier was able to achieve an accuracy of over $90\%$.

\section{Contributions}

The work for the project was balanced between the two members splitting workloads where possible and assigning to make best use of the skills of the individual and using paired programming where appropriate.

\newpage
\section{Appendix}
\begin{appendix}
\section{Run 1 Evaluation Report}
\lstinputlisting{run1.txt}
\newpage
\section{Run 2 Evaluation Report}
\lstinputlisting{run2.txt}
\newpage
\section{Run 3 Evaluation Report}
\lstinputlisting{run3.txt}
\end{appendix}

\end{document}